{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1irI5YaFOkwcn-RxYemgGs2sAu82tr1ej",
      "authorship_tag": "ABX9TyOkvHUyhYF+GCfqvhX3Bb0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoraoui/PFE2022/blob/main/banino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "snmIeFIwqx5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9ac5f3-732d-4a01-d0ff-317a770c7bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_lattice\n",
            "  Downloading tensorflow_lattice-2.0.13-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.0/242.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_lattice) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tensorflow_lattice) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_lattice) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tensorflow_lattice) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tensorflow_lattice) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tensorflow_lattice) (3.2.0)\n",
            "Installing collected packages: tensorflow_lattice\n",
            "Successfully installed tensorflow_lattice-2.0.13\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "!pip install tensorflow_lattice\n",
        "import tensorflow_lattice as tfl\n",
        "import keras\n",
        "from tensorflow.keras.layers import AbstractRNNCell\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Layer, LSTMCell\n",
        "from tensorflow.keras.layers import LSTM\n",
        "task_env_size = 2.2\n",
        "task_neurons_seed = 8341\n",
        "task_targets_type = 'softmax'\n",
        "task_lstm_init_type = 'softmax'\n",
        "task_n_pc = [256]\n",
        "task_pc_scale = [0.01]\n",
        "task_lstm_init_type = 'softmax'\n",
        "task_n_hdc = [12]\n",
        "task_hdc_concentration = [20.]\n",
        "task_dataset_info =  'square_room'\n",
        "task_root = ''\n",
        "training_minibatch_size = 10\n",
        "model_nh_lstm = 128\n",
        "model_nh_bottleneck = 256\n",
        "model_dropout_rates = [0.5]\n",
        "model_weight_decay = 1e-5\n",
        "model_bottleneck_has_bias = False\n",
        "model_init_weight_disp = 0.0\n",
        "nh_lstm = 128\n",
        "\n",
        "task_neurons_seed = 8341\n",
        "rs = np.random.RandomState(task_neurons_seed)\n",
        "concentration = 20\n",
        "stdev = 0.35\n",
        "means_hd = rs.uniform(-np.pi, np.pi, (task_n_hdc[0]))\n",
        "kappa = np.ones_like(means_hd) * concentration\n",
        "pos_min = -5\n",
        "pos_max = 5\n",
        "means_pc = rs.uniform(pos_min, pos_max, size=(task_n_pc[0], 2))\n",
        "variances = np.ones_like(means_pc) * stdev**2\n"
      ],
      "metadata": {
        "id": "iEz-ppQOjjtf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.layers import RNN\n",
        "from keras.src import backend\n",
        "\n",
        "class MinimalRNNCell(AbstractRNNCell):\n",
        "\n",
        "  def __init__(self, units, **kwargs):\n",
        "    self.units = units\n",
        "    super(MinimalRNNCell, self).__init__(**kwargs)\n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return self.units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  initializer='uniform',\n",
        "                                  name='kernel')\n",
        "    self.recurrent_kernel = self.add_weight(\n",
        "        shape=(self.units, self.units),\n",
        "        initializer='uniform',\n",
        "        name='recurrent_kernel')\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, states):\n",
        "    prev_output = states[0]\n",
        "    h = backend.dot(inputs, self.kernel)\n",
        "    output = h + backend.dot(prev_output, self.recurrent_kernel)\n",
        "    return output, output"
      ],
      "metadata": {
        "id": "lJ8x5AMwzTRC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_place_cell_ensembles(\n",
        "    env_size, neurons_seed, targets_type, lstm_init_type, n_pc, pc_scale):\n",
        "  \"\"\"Create the ensembles for the Place cells.\"\"\"\n",
        "  place_cell_ensembles = [\n",
        "        PlaceCellEnsemble(\n",
        "          n,\n",
        "          stdev=s,\n",
        "          pos_min=-env_size / 2.0,\n",
        "          pos_max=env_size / 2.0,\n",
        "          seed=neurons_seed,\n",
        "          soft_targets=targets_type,\n",
        "          soft_init=lstm_init_type)\n",
        "      for n, s in zip(n_pc, pc_scale)\n",
        "  ]\n",
        "  return place_cell_ensembles\n",
        "\n",
        "\n",
        "def get_head_direction_ensembles(\n",
        "    neurons_seed, targets_type, lstm_init_type, n_hdc, hdc_concentration):\n",
        "  \"\"\"Create the ensembles for the Head direction cells.\"\"\"\n",
        "  head_direction_ensembles = [\n",
        "      HeadDirectionCellEnsemble(\n",
        "          n,\n",
        "          concentration=con,\n",
        "          seed=neurons_seed,\n",
        "          soft_targets=targets_type,\n",
        "          soft_init=lstm_init_type)\n",
        "      for n, con in zip(n_hdc, hdc_concentration)\n",
        "  ]\n",
        "  return head_direction_ensembles\n",
        "\n",
        "\n",
        "place_cell_ensembles = get_place_cell_ensembles(\n",
        "  env_size=task_env_size,\n",
        "  neurons_seed=task_neurons_seed,\n",
        "  targets_type=task_targets_type,\n",
        "  lstm_init_type=task_lstm_init_type,\n",
        "  n_pc=task_n_pc,\n",
        "  pc_scale=task_pc_scale)\n",
        "\n",
        "head_direction_ensembles = get_head_direction_ensembles(\n",
        "    neurons_seed=task_neurons_seed,\n",
        "    targets_type=task_targets_type,\n",
        "    lstm_init_type=task_lstm_init_type,\n",
        "    n_hdc=task_n_hdc,\n",
        "    hdc_concentration=task_hdc_concentration)\n"
      ],
      "metadata": {
        "id": "fgfoDWh5kjsj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensors = []\n",
        "init_pos = tf.random.uniform(shape=(10, 2))\n",
        "init_hd = tf.random.uniform(shape=(10, 1))\n",
        "ego_vel = tf.random.uniform(shape=(10, 3))\n",
        "target_pos = tf.random.uniform(shape=(1, 2))\n",
        "target_hd = tf.random.uniform(shape=(1, 1))\n",
        "vel_noise = tfp.distributions.Normal(0.0, 1.0).\\\n",
        "sample(sample_shape=ego_vel.get_shape()) * 0.2\n",
        "input_tensors = [ego_vel + vel_noise] + input_tensors\n",
        "initial_conds = []\n",
        "x_pc = init_pos[:, tf.newaxis, :]\n",
        "diff = x_pc[:, :, tf.newaxis, :] - means_pc[np.newaxis, np.newaxis, ...]\n",
        "y = -0.5 * tf.reduce_sum((diff**2)/ variances, axis=-1)\n",
        "lp = y - tf.reduce_logsumexp(y)\n",
        "print(lp.shape)\n",
        "lp_reshaped = tf.reshape(lp, (256,10))\n",
        "init_pc = tf.nn.softmax(lp_reshaped)\n",
        "init_pc = tf.squeeze(init_pc)\n",
        "\n",
        "x_hd = init_hd[:, tf.newaxis, :]\n",
        "diff = kappa * tf.cos(x_hd - means_hd[np.newaxis, np.newaxis, :])\n",
        "lp = diff - tf.reduce_logsumexp(diff)\n",
        "lp_reshaped = tf.reshape(lp, (12,10))\n",
        "init_hd = tf.nn.softmax(lp_reshaped)\n",
        "init_hd = tf.squeeze(init_hd)\n",
        "\n",
        "\n",
        "x_targ_pc = target_pos\n",
        "diff = x_targ_pc - means_pc\n",
        "y = -0.5 * tf.reduce_sum((diff**2)/ variances, axis=-1)\n",
        "lp = y - tf.reduce_logsumexp(y)\n",
        "init_targ_pos = tf.nn.softmax(lp_reshaped)\n",
        "\n",
        "\n",
        "diff = kappa*tf.cos(target_hd-means_hd[np.newaxis, np.newaxis, :])\n",
        "lp = y - tf.reduce_logsumexp(diff)\n",
        "init_targ_hd = tf.nn.softmax(lp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_06-4OZ32y3",
        "outputId": "93b23bd6-ca31-4b32-d06b-172c04554e4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Ensembles of place and head direction cells.\n",
        "\n",
        "These classes provide the targets for the training of grid-cell networks.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def one_hot_max(x, axis=-1):\n",
        "  \"\"\"Compute one-hot vectors setting to one the index with the maximum value.\"\"\"\n",
        "  return tf.one_hot(tf.argmax(x, axis=axis),\n",
        "                    depth=x.get_shape()[-1],\n",
        "                    dtype=x.dtype)\n",
        "\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "  \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "  return tf.nn.softmax(x, dim=axis)\n",
        "\n",
        "\n",
        "def softmax_sample(x):\n",
        "  \"\"\"Sample the categorical distribution from logits and sample it.\"\"\"\n",
        "  dist = tf.contrib.distributions.OneHotCategorical(logits=x, dtype=tf.float32)\n",
        "  return dist.sample()\n",
        "\n",
        "\n",
        "class CellEnsemble(object):\n",
        "  \"\"\"Abstract parent class for place and head direction cell ensembles.\"\"\"\n",
        "\n",
        "  def __init__(self, n_cells, soft_targets, soft_init):\n",
        "    self.n_cells = n_cells\n",
        "    if soft_targets not in [\"softmax\", \"voronoi\", \"sample\", \"normalized\"]:\n",
        "      raise ValueError\n",
        "    else:\n",
        "      self.soft_targets = soft_targets\n",
        "    # Provide initialization of LSTM in the same way as targets if not specified\n",
        "    # i.e one-hot if targets are Voronoi\n",
        "    if soft_init is None:\n",
        "      self.soft_init = soft_targets\n",
        "    else:\n",
        "      if soft_init not in [\n",
        "          \"softmax\", \"voronoi\", \"sample\", \"normalized\", \"zeros\"\n",
        "      ]:\n",
        "        raise ValueError\n",
        "      else:\n",
        "        self.soft_init = soft_init\n",
        "\n",
        "  def get_targets(self, x):\n",
        "    \"\"\"Type of target.\"\"\"\n",
        "\n",
        "    if self.soft_targets == \"normalized\":\n",
        "      targets = tf.exp(self.unnor_logpdf(x))\n",
        "    elif self.soft_targets == \"softmax\":\n",
        "      lp = self.log_posterior(x)\n",
        "      targets = softmax(lp)\n",
        "    elif self.soft_targets == \"sample\":\n",
        "      lp = self.log_posterior(x)\n",
        "      targets = softmax_sample(lp)\n",
        "    elif self.soft_targets == \"voronoi\":\n",
        "      lp = self.log_posterior(x)\n",
        "      targets = one_hot_max(lp)\n",
        "    return targets\n",
        "\n",
        "  def get_init(self, x):\n",
        "    \"\"\"Type of initialisation.\"\"\"\n",
        "\n",
        "    if self.soft_init == \"normalized\":\n",
        "      init = tf.exp(self.unnor_logpdf(x))\n",
        "    elif self.soft_init == \"softmax\":\n",
        "      lp = self.log_posterior(x)\n",
        "      init = softmax(lp)\n",
        "    elif self.soft_init == \"sample\":\n",
        "      lp = self.log_posterior(x)\n",
        "      init = softmax_sample(lp)\n",
        "    elif self.soft_init == \"voronoi\":\n",
        "      lp = self.log_posterior(x)\n",
        "      init = one_hot_max(lp)\n",
        "    elif self.soft_init == \"zeros\":\n",
        "      init = tf.zeros_like(self.unnor_logpdf(x))\n",
        "    return init\n",
        "\n",
        "  def loss(self, predictions, targets):\n",
        "    \"\"\"Loss.\"\"\"\n",
        "\n",
        "    if self.soft_targets == \"normalized\":\n",
        "      smoothing = 1e-2\n",
        "      loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=(1. - smoothing) * targets + smoothing * 0.5,logits=predictions,name=\"ensemble_loss\")\n",
        "      loss = tf.reduce_mean(loss, axis=-1)\n",
        "    else:\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          labels=targets,\n",
        "          logits=predictions,\n",
        "          name=\"ensemble_loss\")\n",
        "    return loss\n",
        "\n",
        "  def log_posterior(self, x):\n",
        "    logp = self.unnor_logpdf(x)\n",
        "    log_posteriors = logp - tf.reduce_logsumexp(logp, axis=2, keep_dims=True)\n",
        "    return log_posteriors\n",
        "\n",
        "\n",
        "class PlaceCellEnsemble(CellEnsemble):\n",
        "  \"\"\"Calculates the dist over place cells given an absolute position.\"\"\"\n",
        "\n",
        "  def __init__(self, n_cells, stdev=0.35, pos_min=-5, pos_max=5, seed=None,\n",
        "               soft_targets=None, soft_init=None):\n",
        "    super(PlaceCellEnsemble, self).__init__(n_cells, soft_targets, soft_init)\n",
        "    # Create a random MoG with fixed cov over the position (Nx2)\n",
        "    rs = np.random.RandomState(seed)\n",
        "    self.means = rs.uniform(pos_min, pos_max, size=(self.n_cells, 2))\n",
        "    self.variances = np.ones_like(self.means) * stdev**2\n",
        "\n",
        "  def unnor_logpdf(self, trajs):\n",
        "    # Output the probability of each component at each point (BxTxN)\n",
        "    diff = trajs[:, :, tf.newaxis, :] - self.means[np.newaxis, np.newaxis, ...]\n",
        "    unnor_logp = -0.5 * tf.reduce_sum((diff**2)/ self.variances, axis=-1)\n",
        "    return unnor_logp\n",
        "  def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
        "    return super().__call__(*args, **kwds)\n",
        "\n",
        "\n",
        "class HeadDirectionCellEnsemble(CellEnsemble):\n",
        "  \"\"\"Calculates the dist over HD cells given an absolute angle.\"\"\"\n",
        "\n",
        "  def __init__(self, n_cells, concentration=20, seed=None,\n",
        "               soft_targets=None, soft_init=None):\n",
        "    super(HeadDirectionCellEnsemble, self).__init__(n_cells,\n",
        "                                                    soft_targets,\n",
        "                                                    soft_init)\n",
        "    # Create a random Von Mises with fixed cov over the position\n",
        "    rs = np.random.RandomState(seed)\n",
        "    self.means = rs.uniform(-np.pi, np.pi, (n_cells))\n",
        "    self.kappa = np.ones_like(self.means) * concentration\n",
        "\n",
        "  def unnor_logpdf(self, x):\n",
        "    return self.kappa * tf.cos(x - self.means[np.newaxis, np.newaxis, :])\n",
        "  def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
        "    return super().__call__(*args, **kwds)\n",
        "\n"
      ],
      "metadata": {
        "id": "LDPivVma3cCM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN layer can take a list of cells, which will then stack them together.\n",
        "# By default, keras RNN will only return the last timestep output and will not\n",
        "# return states. If you need whole time sequence output as well as the states,\n",
        "# you can set `return_sequences` and `return_state` to True.\n",
        "rnn_layer = tf.keras.layers.RNN([tf.keras.layers.LSTMCell(128),\n",
        "                                 tf.keras.layers.LSTMCell(256)],\n",
        "                                return_sequences=True,\n",
        "                                return_state=True)\n",
        "outputs, output_states = rnn_layer(inputs, states)"
      ],
      "metadata": {
        "id": "094O7t7KrKxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_env_size = 2.2\n",
        "task_neurons_seed = 8341\n",
        "task_targets_type = 'softmax'\n",
        "task_lstm_init_type = 'softmax'\n",
        "task_n_pc = [256]\n",
        "task_pc_scale = [0.01]\n",
        "task_lstm_init_type = 'softmax'\n",
        "task_n_hdc = [12]\n",
        "task_hdc_concentration = [20.]\n",
        "task_dataset_info =  'square_room'\n",
        "task_root = ''\n",
        "training_minibatch_size = 10\n",
        "model_nh_lstm = 128\n",
        "model_nh_bottleneck = 256\n",
        "model_dropout_rates = [0.5]\n",
        "model_weight_decay = 1e-5\n",
        "model_bottleneck_has_bias = False\n",
        "model_init_weight_disp = 0.0\n",
        "\n",
        "rnn_core = GridCellsRNNCell(\n",
        "  target_ensembles=target_ensembles,\n",
        "  nh_lstm=model_nh_lstm,\n",
        "  nh_bottleneck=model_nh_bottleneck,\n",
        "  dropoutrates_bottleneck=np.array(model_dropout_rates),\n",
        "  bottleneck_weight_decay=model_weight_decay,\n",
        "  bottleneck_has_bias=model_bottleneck_has_bias,\n",
        "  init_weight_disp=model_init_weight_disp)\n"
      ],
      "metadata": {
        "id": "WnRwzjxJ2rNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "dd5dcbb9-7158-4357-9708-62a1fa9d8302"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-782e3095dfd9>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel_init_weight_disp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m rnn_core = GridCellsRNNCell(\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mtarget_ensembles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ensembles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mnh_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_nh_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GridCellsRNNCell' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "place_cell_ensembles = get_place_cell_ensembles(\n",
        "  env_size=task_env_size,\n",
        "  neurons_seed=task_neurons_seed,\n",
        "  targets_type=task_targets_type,\n",
        "  lstm_init_type=task_lstm_init_type,\n",
        "  n_pc=task_n_pc,\n",
        "  pc_scale=task_pc_scale)\n",
        "\n",
        "head_direction_ensembles = get_head_direction_ensembles(\n",
        "    neurons_seed=task_neurons_seed,\n",
        "    targets_type=task_targets_type,\n",
        "    lstm_init_type=task_lstm_init_type,\n",
        "    n_hdc=task_n_hdc,\n",
        "    hdc_concentration=task_hdc_concentration)\n",
        "\n",
        "\n",
        "target_ensembles = place_cell_ensembles + head_direction_ensembles\n",
        "rnn_core = GridCellsRNNCell(\n",
        "  target_ensembles=target_ensembles,\n",
        "  nh_lstm=model_nh_lstm,\n",
        "  nh_bottleneck=model_nh_bottleneck,\n",
        "  dropoutrates_bottleneck=np.array(model_dropout_rates),\n",
        "  bottleneck_weight_decay=model_weight_decay,\n",
        "  bottleneck_has_bias=model_bottleneck_has_bias,\n",
        "  init_weight_disp=model_init_weight_disp)\n"
      ],
      "metadata": {
        "id": "3hdD49wk30jS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "d294dbe2-1194-49d0-a2b9-da2241a1333d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f559e1afc7d9>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtarget_ensembles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplace_cell_ensembles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhead_direction_ensembles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m rnn_core = GridCellsRNNCell(\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mtarget_ensembles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ensembles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mnh_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_nh_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GridCellsRNNCell' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs_pc = tf.keras.Input(shape=(init_pc.shape[0]*init_pc.shape[1],))\n",
        "d = tf.keras.layers.Dense(init_pc.shape[0]*init_pc.shape[1])\n",
        "x_pc = d(inputs_pc)\n",
        "\n",
        "inputs_hd = tf.keras.Input(shape=(init_hd.shape[0]*init_hd.shape[1],))\n",
        "d = tf.keras.layers.Dense(init_hd.shape[0]*init_hd.shape[1])\n",
        "x_hd = d(inputs_hd)\n",
        "initialization_pc = tf.keras.layers.Dense(128)(x_pc)\n",
        "initialization_hd = tf.keras.layers.Dense(128)(x_hd)\n",
        "\n",
        "layer_pc = tfl.layers.Linear(num_input_dims=init_pc.shape[0]*init_pc.shape[1],units=128)\n",
        "layer_hd = tfl.layers.Linear(num_input_dims=init_hd.shape[0]*init_hd.shape[1],units=128)\n",
        "ego_vel = tf.random.uniform(shape=(10, 10, 3))\n",
        "number_of_units=128\n",
        "n_time_steps, n_features=10, 10\n",
        "\n",
        "# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
        "initialization = tf.concat([initialization_pc, initialization_hd], 0)\n",
        "\n",
        "# Assuming you have a SimpleRNNCell defined as rnn_core\n",
        "rnn_cell = MinimalRNNCell(units=128)\n",
        "\n",
        "# 'ego_vel' is a tensor of shape [batch_size, n_time_steps, n_features]\n",
        "ego_vel = tf.random.uniform(shape=(35, 10, 3))  # Adjust batch_size as needed\n",
        "\n",
        "# 'initialization_pc' is a tensor of shape [batch_size, 128]\n",
        "\n",
        "# Use tf.keras.layers.RNN wrapper\n",
        "rnn_layer = tf.keras.layers.RNN(rnn_cell, return_sequences=True, return_state=True)\n",
        "\n",
        "# 'state' is a tensor of shape [batch_size, cell_state_size]\n",
        "outputs, state = rnn_layer(ego_vel, initial_state=[initialization])\n",
        "\n"
      ],
      "metadata": {
        "id": "h7nQV5SbM0Ad"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialization = tf.concat([initialization_pc, initialization_hd], 0)"
      ],
      "metadata": {
        "id": "2UTlbx6p0x0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new class of RNN Cell\n"
      ],
      "metadata": {
        "id": "58Y_31artzAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridCellsRNNCell(keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               target_ensembles,\n",
        "               nh_lstm,\n",
        "               nh_bottleneck,\n",
        "               nh_embed=None,\n",
        "               dropoutrates_bottleneck=None,\n",
        "               bottleneck_weight_decay=0.0,\n",
        "               bottleneck_has_bias=False,\n",
        "               init_weight_disp=0.0,\n",
        "               name=\"grid_cells_core\",  **kwargs):\n",
        "    self._lstm = LSTM(nh_lstm)\n",
        "    self.state_size = 128\n",
        "    super(GridCellsRNNCell, self).__init__(**kwargs)\n",
        "\n",
        "    self._target_ensembles = target_ensembles\n",
        "    self._nh_embed = nh_embed\n",
        "    self._nh_lstm = nh_lstm\n",
        "    self._nh_bottleneck = nh_bottleneck #256\n",
        "    self._dropoutrates_bottleneck = dropoutrates_bottleneck\n",
        "    self._bottleneck_weight_decay = bottleneck_weight_decay\n",
        "    self._bottleneck_has_bias = bottleneck_has_bias\n",
        "    self._init_weight_disp = init_weight_disp\n",
        "    #self.training = False\n",
        "\n",
        "  def __call__(self, inputs, prev_state):\n",
        "    conc_inputs = tf.concat(inputs, axis=1, name=\"conc_inputs\")\n",
        "    lstm_inputs = conc_inputs\n",
        "    lstm_output, next_state = self._lstm(lstm_inputs, prev_state)\n",
        "    bottleneck = Dense(self._nh_bottleneck,\n",
        "                            use_bias=self._bottleneck_has_bias,\n",
        "                            regularizers={\n",
        "                                \"w\": tf.contrib.layers.l2_regularizer(\n",
        "                                    self._bottleneck_weight_decay)},\n",
        "                            name=\"bottleneck\")(lstm_output)\n",
        "\n",
        "\n",
        "    if self.training and self._dropoutrates_bottleneck is not None:\n",
        "      tf.logging.info(\"Adding dropout layers\")\n",
        "      n_scales = len(self._dropoutrates_bottleneck)  #model_dropout_rates=0.5\n",
        "      scale_pops = tf.split(bottleneck, n_scales, axis=1)\n",
        "      dropped_pops = [tf.nn.dropout(pop, rate, name=\"dropout\")\n",
        "                      for rate, pop in zip(self._dropoutrates_bottleneck,\n",
        "                                           scale_pops)]\n",
        "      bottleneck = tf.concat(dropped_pops, axis=1)\n",
        "    # Outputs\n",
        "    ens_outputs = [snt.Linear(\n",
        "        ens.n_cells,\n",
        "        regularizers={\n",
        "            \"w\": tf.contrib.layers.l2_regularizer(\n",
        "                self._bottleneck_weight_decay)},\n",
        "        initializers={\n",
        "            \"w\": displaced_linear_initializer(self._nh_bottleneck,\n",
        "                                              self._init_weight_disp,\n",
        "                                              dtype=tf.float32)},\n",
        "        name=\"pc_logits\")(bottleneck)\n",
        "                   for ens in self._target_ensembles]\n",
        "    return (ens_outputs, bottleneck, lstm_output), tuple(list(next_state))\n",
        "  def get_initial_state(self, batch_size, dtype):\n",
        "    # Your custom get_initial_state implementation here\n",
        "    # Return a tensor with the appropriate shape and dtype\n",
        "    initial_state = tf.zeros([batch_size, self.state_size], dtype=dtype)\n",
        "    return initial_state\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    \"\"\"Returns a description of the output size, without batch dimension.\"\"\"\n",
        "    return tuple([ens.n_cells for ens in self._target_ensembles] +\n",
        "                 [self._nh_bottleneck, self._nh_lstm])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jQ9HUGZn1I1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.layers import RNN, LSTM\n",
        "from keras.src import backend\n",
        "\n",
        "# First, let's define a RNN Cell, as a layer subclass.\n",
        "class GridRNNCell(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, **kwargs):\n",
        "        self.units = units\n",
        "        self.state_size =  (self.units, self.units)\n",
        "        self._lstm = tf.keras.layers.LSTM(32)\n",
        "        super(GridRNNCell, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, inputs, prev_state):\n",
        "        #conc_inputs = tf.concat(inputs, axis=1, name=\"conc_inputs\")\n",
        "        lstm = LSTM(32, return_state=True)(inputs)\n",
        "        y = lstm(inputs, initial_state=prev_state)\n",
        "        return None\n",
        "        return lstm_output\n",
        "\n",
        "\n",
        "# Let's use this cell in a RNN layer:\n",
        "x = tf.keras.Input(shape = (10, 5))  # 10 timesteps, input dim 5\n",
        "# Define previous state tensors\n",
        "previous_hidden_state = tf.keras.Input((32,))\n",
        "previous_cell_state = tf.keras.Input((32,))\n",
        "previous_state = [previous_hidden_state, previous_cell_state]\n",
        "cell = GridRNNCell(32)\n",
        "layer = RNN(cell)\n",
        "y = layer(x, previous_state)\n"
      ],
      "metadata": {
        "id": "gWiP6vj-f7Dv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}